{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估AI模型：代码、人工和基于模型的评分\n",
    "\n",
    "在本笔记本中，我们将深入探讨三种广泛使用的评估AI模型（如Claude v3）效果的技术：\n",
    "\n",
    "1. 基于代码的评分\n",
    "2. 人工评分\n",
    "3. 基于模型的评分\n",
    "\n",
    "我们将通过示例说明每种方法，并在评估AI性能时检查它们各自的优势和局限性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于代码的评分示例：情感分析\n",
    "\n",
    "在这个示例中，我们将评估 Claude 将电影评论情感分类为“积极”或“消极”的能力。我们可以通过代码检查模型输出是否与预期情感一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the model name and AWS region for later use\n",
    "MODEL_NAME = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "AWS_REGION = \"us-west-2\"\n",
    "\n",
    "%store MODEL_NAME\n",
    "%store AWS_REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install the Anthropic package\n",
    "%pip install anthropic --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the AnthropicBedrock class and create a client instance\n",
    "from anthropic import AnthropicBedrock\n",
    "\n",
    "client = AnthropicBedrock(aws_region=AWS_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build the input prompt for sentiment analysis\n",
    "def build_input_prompt(review):\n",
    "    user_content = f\"\"\"Classify the sentiment of the following movie review as either 'positive' or 'negative' provide only one of those two choices:\n",
    "    <review>{review}</review>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Define the evaluation data\n",
    "eval = [\n",
    "    {\n",
    "        \"review\": \"This movie was amazing! The acting was superb and the plot kept me engaged from start to finish.\",\n",
    "        \"golden_answer\": \"positive\"\n",
    "    },\n",
    "    {\n",
    "        \"review\": \"I was thoroughly disappointed by this film. The pacing was slow and the characters were one-dimensional.\",\n",
    "        \"golden_answer\": \"negative\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to get completions from the model\n",
    "def get_completion(messages):\n",
    "    message = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "        messages=messages\n",
    "    )\n",
    "    return message.content[0].text\n",
    "\n",
    "# Get completions for each input\n",
    "outputs = [get_completion(build_input_prompt(item[\"review\"])) for item in eval]\n",
    "\n",
    "# Print the outputs and golden answers\n",
    "for output, question in zip(outputs, eval):\n",
    "    print(f\"Review: {question['review']}\\nGolden Answer: {question['golden_answer']}\\nOutput: {output}\\n\")\n",
    "\n",
    "# Function to grade the completions\n",
    "def grade_completion(output, golden_answer):\n",
    "    return output.lower() == golden_answer.lower()\n",
    "\n",
    "# Grade the completions and print the accuracy\n",
    "grades = [grade_completion(output, item[\"golden_answer\"]) for output, item in zip(outputs, eval)]\n",
    "print(f\"Accuracy: {sum(grades) / len(grades) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 人工评分示例：作文打分\n",
    "\n",
    "有些任务（例如作文打分）难以仅用代码客观评估。在这种情况下，我们可以为人工评审提供评分指引，以评估模型的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build the input prompt for essay generation\n",
    "def build_input_prompt(topic):\n",
    "    user_content = f\"\"\"Write a short essay discussing the following topic:\n",
    "    <topic>{topic}</topic>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Define the evaluation data\n",
    "eval = [\n",
    "    {\n",
    "        \"topic\": \"The importance of education in personal development and societal progress\",\n",
    "        \"golden_answer\": \"A high-scoring essay should have a clear thesis, well-structured paragraphs, and persuasive examples discussing how education contributes to individual growth and broader societal advancement.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get completions for each input\n",
    "outputs = [get_completion(build_input_prompt(item[\"topic\"])) for item in eval]\n",
    "\n",
    "# Print the outputs and golden answers\n",
    "for output, item in zip(outputs, eval):\n",
    "    print(f\"Topic: {item['topic']}\\n\\nGrading Rubric:\\n {item['golden_answer']}\\n\\nModel Output:\\n{output}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于模型的评分示例\n",
    "\n",
    "我们可以把模型的回答与评分量表一并提供给 Claude，让它对自己的输出进行评分。这样就能自动化评估那些通常需要人工判断的任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例 1：摘要\n",
    "\n",
    "在这个示例中，我们将使用 Claude 评估它自己生成的摘要质量。这在需要评估模型能否简洁准确地抓住长文本关键信息时很有用。通过提供应覆盖要点的评分量表，我们可以自动化打分流程，并快速评估模型在摘要任务上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build the input prompt for summarization\n",
    "def build_input_prompt(text):\n",
    "    user_content = f\"\"\"Please summarize the main points of the following text:\n",
    "    <text>{text}</text>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Function to build the grader prompt for assessing summary quality\n",
    "def build_grader_prompt(output, rubric):\n",
    "    user_content = f\"\"\"Assess the quality of the following summary based on this rubric:\n",
    "    <rubric>{rubric}</rubric>\n",
    "    <summary>{output}</summary>\n",
    "    Provide a score from 1-5, where 1 is poor and 5 is excellent.\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Define the evaluation data\n",
    "eval = [\n",
    "    {\n",
    "        \"text\": \"The Magna Carta, signed in 1215, was a pivotal document in English history. It limited the powers of the monarchy and established the principle that everyone, including the king, was subject to the law. This laid the foundation for constitutional governance and the rule of law in England and influenced legal systems worldwide.\",\n",
    "        \"golden_answer\": \"A high-quality summary should concisely capture the key points: 1) The Magna Carta's significance in English history, 2) Its role in limiting monarchical power, 3) Establishing the principle of rule of law, and 4) Its influence on legal systems around the world.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get completions for each input\n",
    "outputs = [get_completion(build_input_prompt(item[\"text\"])) for item in eval]\n",
    "\n",
    "# Grade the completions\n",
    "grades = [get_completion(build_grader_prompt(output, item[\"golden_answer\"])) for output, item in zip(outputs, eval)]\n",
    "\n",
    "# Print the summary quality score\n",
    "print(f\"Summary quality score: {grades[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例 2：事实核查\n",
    "\n",
    "在这个示例中，我们将使用 Claude 对一个断言进行事实核查，并评估它核查的准确性。这有助于评估模型区分正确信息与错误信息的能力。通过提供正确事实核查应涵盖要点的量表，我们可以自动化评分流程并快速评估模型在事实核查任务上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build the input prompt for fact-checking\n",
    "def build_input_prompt(claim):\n",
    "    user_content = f\"\"\"Determine if the following claim is true or false:\n",
    "    <claim>{claim}</claim>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Function to build the grader prompt for assessing fact-check accuracy\n",
    "def build_grader_prompt(output, rubric):\n",
    "    user_content = f\"\"\"Based on the following rubric, assess whether the fact-check is correct:\n",
    "    <rubric>{rubric}</rubric>\n",
    "    <fact-check>{output}</fact-check>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Define the evaluation data\n",
    "eval = [\n",
    "    {\n",
    "        \"claim\": \"The Great Wall of China is visible from space.\",\n",
    "        \"golden_answer\": \"A correct fact-check should state that this claim is false. While the Great Wall is an impressive structure, it is not visible from space with the naked eye.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get completions for each input\n",
    "outputs = [get_completion(build_input_prompt(item[\"claim\"])) for item in eval]\n",
    "\n",
    "grades = []\n",
    "for output, item in zip(outputs, eval):\n",
    "    # Print the claim, fact-check, and rubric\n",
    "    print(f\"Claim: {item['claim']}\\n\")\n",
    "    print(f\"Fact-check: {output}]\\n\")\n",
    "    print(f\"Rubric: {item['golden_answer']}\\n\")\n",
    "    \n",
    "    # Grade the fact-check\n",
    "    grader_prompt = build_grader_prompt(output, item[\"golden_answer\"])\n",
    "    grade = get_completion(grader_prompt)\n",
    "    grades.append(\"correct\" in grade.lower())\n",
    "\n",
    "# Print the fact-checking accuracy\n",
    "accuracy = sum(grades) / len(grades)\n",
    "print(f\"Fact-checking accuracy: {accuracy * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例 3：语气分析\n",
    "\n",
    "在这个示例中，我们将使用 Claude 分析一段文本的语气，并评估其分析的准确性。这有助于评估模型识别和解读文本情绪与态度的能力。通过提供需要识别的语气关键要素的量表，我们可以自动化评分流程，并快速评估模型在语气分析任务上的表现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to build the input prompt for tone analysis\n",
    "def build_input_prompt(text):\n",
    "    user_content = f\"\"\"Analyze the tone of the following text:\n",
    "    <text>{text}</text>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Function to build the grader prompt for assessing tone analysis accuracy\n",
    "def build_grader_prompt(output, rubric):\n",
    "    user_content = f\"\"\"Assess the accuracy of the following tone analysis based on this rubric:\n",
    "    <rubric>{rubric}</rubric>\n",
    "    <tone-analysis>{output}</tone-analysis>\"\"\"\n",
    "    return [{\"role\": \"user\", \"content\": user_content}]\n",
    "\n",
    "# Define the evaluation data\n",
    "eval = [\n",
    "    {\n",
    "        \"text\": \"I can't believe they canceled the event at the last minute. This is completely unacceptable and unprofessional!\",\n",
    "        \"golden_answer\": \"The tone analysis should identify the text as expressing frustration, anger, and disappointment. Key words like 'can't believe', 'last minute', 'unacceptable', and 'unprofessional' indicate strong negative emotions.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Get completions for each input\n",
    "outputs = [get_completion(build_input_prompt(item[\"text\"])) for item in eval]\n",
    "\n",
    "# Grade the completions\n",
    "grades = [get_completion(build_grader_prompt(output, item[\"golden_answer\"])) for output, item in zip(outputs, eval)]\n",
    "\n",
    "# Print the tone analysis quality\n",
    "print(f\"Tone analysis quality: {grades[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上示例展示了如何通过基于代码、人工以及基于模型的评分来评估像 Claude 这样的 AI 模型在各类任务上的表现。具体采用哪种评估方法，取决于任务的性质与可用资源。基于模型的评分为自动化评估那些原本需要人工判断的复杂任务提供了一种很有前景的路径。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
